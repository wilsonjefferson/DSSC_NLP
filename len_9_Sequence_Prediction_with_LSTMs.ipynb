{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "len_9_Sequence_Prediction_with_LSTMs.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1cD1TCuCnaW"
      },
      "source": [
        "# Sequence Prediction with (Bi-)LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J_Z3P3vCnaX"
      },
      "source": [
        "## Data\n",
        "\n",
        "We need to get some data. We use the same CoNLL reader function as we had for the Structured Perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0MHzgJCnaX"
      },
      "source": [
        "def read_conll_file(file_name):\n",
        "    \"\"\"\n",
        "    read in a file with CoNLL format:\n",
        "    word1    tag1\n",
        "    word2    tag2\n",
        "    ...      ...\n",
        "    wordN    tagN\n",
        "\n",
        "    Sentences MUST be separated by newlines!\n",
        "    \"\"\"\n",
        "    current_words = []\n",
        "    current_tags = []\n",
        "\n",
        "    with open(file_name, encoding='utf-8') as conll:\n",
        "        for line in conll:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line:\n",
        "                word, tag = line.split('\\t')\n",
        "                current_words.append(word)\n",
        "                current_tags.append(tag)\n",
        "\n",
        "            else:\n",
        "                yield (current_words, current_tags)\n",
        "                current_words = []\n",
        "                current_tags = []\n",
        "\n",
        "    # if file does not end in newline (it should...), check whether there is an instance in the buffer\n",
        "    if current_tags != []:\n",
        "        yield (current_words, current_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62sqRbyACnaa"
      },
      "source": [
        "Now we need to do some bookkeeping: collect the set of known words and tags, map both words and tags into integer indices, and keep track of the mapping. We also need two special tokens: one for unknown words, the other one for padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdBCWuhECnaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a8da4697-8023-40d7-cd63-dde1c6a67c4f"
      },
      "source": [
        "# collect known word tokens and tags\n",
        "wordset, tagset = set(), set()\n",
        "train_instances = [(words, tags) for (words, tags) in read_conll_file('tiny_POS_train.data')]\n",
        "for (words, tags) in train_instances:\n",
        "    tagset.update(set(tags))\n",
        "    wordset.update(set(words))\n",
        "\n",
        "# map words and tags into ints\n",
        "PAD = '-PAD-'\n",
        "UNK = '-UNK-'\n",
        "word2int = {word: i + 2 for i, word in enumerate(sorted(wordset))}\n",
        "word2int[PAD] = 0  # special token for padding\n",
        "word2int[UNK] = 1  # special token for unknown words\n",
        " \n",
        "tag2int = {tag: i + 1 for i, tag in enumerate(sorted(tagset))}\n",
        "tag2int[PAD] = 0\n",
        "# to translate it back\n",
        "int2tag = {i:tag for tag, i in tag2int.items()}\n",
        "\n",
        "\n",
        "def convert2ints(instances):\n",
        "    result = []\n",
        "    for (words, tags) in instances:\n",
        "        # replace words with int, 1 for unknown words\n",
        "        word_ints = [word2int.get(word, 1) for word in words]\n",
        "        # replace tags with int\n",
        "        tag_ints = [tag2int[tag] for tag in tags]\n",
        "        result.append((word_ints, tag_ints))\n",
        "    return result        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f38a59d8d816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# collect known word tokens and tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwordset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_conll_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tiny_POS_train.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_instances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtagset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-f38a59d8d816>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# collect known word tokens and tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwordset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_conll_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tiny_POS_train.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_instances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtagset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e44711f952bd>\u001b[0m in \u001b[0;36mread_conll_file\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcurrent_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiny_POS_train.data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcyPpDbxi787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e11ba9-2d20-41cc-e61d-c927bff66797"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g5RQJt7dgBsvNCcYJ-vykjlfQsvTDfadGO61O825kltJegusrJ2GMo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRTVskCgCnac"
      },
      "source": [
        "# get some test data\n",
        "test_instances = [(words, tags) for (words, tags) in read_conll_file('tiny_POS_test.data')]\n",
        "\n",
        "# apply integer mapping\n",
        "train_instances_int = convert2ints(train_instances)\n",
        "test_instances_int = convert2ints(test_instances)\n",
        "\n",
        "# separate the words from the tags\n",
        "train_sentences, train_tags = zip(*train_instances_int) \n",
        "test_sentences, test_tags = zip(*test_instances_int) \n",
        "\n",
        "print(train_instances[0][0])\n",
        "print(train_sentences[0])\n",
        "print(train_instances[0][1])\n",
        "print(train_tags[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-4TpH2Cnaf"
      },
      "source": [
        "Even though RNNs theoretically work for any length input, in practice, we need to make every sentence the same length. We choose the longest training sentence, add some extra, and then pad everything up to that length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT2Qv2i3Cnaf"
      },
      "source": [
        "# get longest training sentence and add 5\n",
        "MAX_LENGTH = len(max(train_sentences, key=len)) + 5\n",
        "print(MAX_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hczp_Iy5Cnah"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Instead of the longest sentence, find the 90th percentile of all training sentence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON6FRRndCnal"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4m1jKIRDzsk"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOuCkkRBCnan"
      },
      "source": [
        "`Keras` provides a function to do the padding for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvFjaA5ZCnao"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "# add special padding at the end of every instance, up to MAX_LENGTH\n",
        "train_sentences = pad_sequences(train_sentences, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences = pad_sequences(test_sentences, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags = pad_sequences(train_tags, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags = pad_sequences(test_tags, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "print(train_sentences[0])\n",
        "print(train_tags[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdCkGMSmCnaq"
      },
      "source": [
        "## Model\n",
        "\n",
        "We use the [functional API](https://keras.io/getting-started/functional-api-guide/) in `keras`. Each layer is a function, which takes as input the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaYzHBxGCnaq"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding\n",
        "from keras.layers import Bidirectional, LSTM\n",
        "from keras.layers import Dropout, Dense, Activation\n",
        "import numpy as np\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "inputs = Input((MAX_LENGTH, ), \n",
        "               name='word_IDs')\n",
        "embeddings = Embedding(input_dim=len(word2int), \n",
        "                       output_dim=128, \n",
        "                       mask_zero=True, \n",
        "                       name='embeddings')(inputs)\n",
        "lstm = LSTM(units=256,\n",
        "              return_sequences=True,\n",
        "              name=\"LSTM\")(embeddings)\n",
        "dropout = Dropout(0.3, name='dropout')(lstm)\n",
        "lstm_out = Dense(len(tag2int), name='output')(dropout)\n",
        "output = Activation('softmax', name='softmax')(lstm_out)\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8olJh8v2Cnas"
      },
      "source": [
        "In order to read the tags, the model needs 1-hot encoding, i.e., a vector with one dimension for each tag, all of them 0, except the one dimension corresponding to the tag we have. Lucikly, there exists a utility function for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BH7-gFVCnas"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_tags_1hot = to_categorical(train_tags, len(tag2int))\n",
        "test_tags_1hot = to_categorical(test_tags, len(tag2int))\n",
        "\n",
        "# originally 50 tag IDs\n",
        "print(train_tags[0])\n",
        "# now 50 rows with 13 columns\n",
        "print(train_tags_1hot[0].shape)\n",
        "# the 1-hot encoding of tag ID 7\n",
        "print(train_tags_1hot[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpmhuhssCnau"
      },
      "source": [
        "Now we can train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtAQG5W_Cnau"
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 5\n",
        "\n",
        "# compile the model we have defined above\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy']\n",
        "             )\n",
        "\n",
        "# run training and capture ouput log\n",
        "history = model.fit(train_sentences, train_tags_1hot,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZGTuDDUCnaw"
      },
      "source": [
        "We can plot the performance on training and dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KMlC6gyCnax"
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "\n",
        "df = pd.DataFrame(history.history)\n",
        "df[['val_accuracy', 'accuracy']].plot.line();\n",
        "df[['val_loss', 'loss']].plot.line();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyPKfXFJCnay"
      },
      "source": [
        "Let's see how well we do on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FabCH5rCnaz"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_sentences, test_tags_1hot,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osd7HaylCna0"
      },
      "source": [
        "Not too bad! Let's see how well it does with sentences that have ambiguous words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZzkB63zCna1"
      },
      "source": [
        "strange_sentences = ['Their management plan reforms worked'.split(),\n",
        "                     \"Attack was their best option\".split()\n",
        "                    ]\n",
        "# convert to integers\n",
        "strange_sentences_int = [[word2int.get(word, 1) for word in sentence] for sentence in strange_sentences]\n",
        "# add padding\n",
        "strange_sentences_int = pad_sequences(strange_sentences_int, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "predictions = model.predict(strange_sentences_int)\n",
        "print(predictions, predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZngTT5RiCna3"
      },
      "source": [
        "Note some things:\n",
        "1. The output for each word is a probability distribution over tags. Nice if we want it, but here, we are actually more interested in the one **best** tag for each token.\n",
        "2. the tags are the integer IDs, so we need to translate them back\n",
        "3. because we padded, the last item just gets repeated\n",
        "    \n",
        "We need to fix those before we have useful output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQCPcU7PCna4"
      },
      "source": [
        "def inverse_transform(sentences, predictions):\n",
        "    output = []\n",
        "    for sentence, prediction in zip(sentences, predictions):\n",
        "        # find the index of the highest-scoring tag and translate it back\n",
        "        token_sequence = [int2tag[np.argmax(prediction[i])] for i in range(len(sentence))]\n",
        "        output.append(token_sequence)\n",
        "    return output\n",
        "\n",
        "print(list(zip(strange_sentences, inverse_transform(strange_sentences, predictions))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ktoO9gzCna5"
      },
      "source": [
        "Looks ok! Let's see what we get if we process the sentence from both ends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15kX9LXECna6"
      },
      "source": [
        "## Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-cvOnK9Cna6"
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "inputs = Input((MAX_LENGTH, ), \n",
        "               name='word_IDs')\n",
        "embeddings = Embedding(input_dim=len(word2int), \n",
        "                       output_dim=128, \n",
        "                       mask_zero=True, \n",
        "                       name='embeddings')(inputs)\n",
        "#wrap the LSTM in a Bidirectional wrapper\n",
        "bilstm = Bidirectional(LSTM(units=256, \n",
        "                            return_sequences=True), \n",
        "                       name=\"Bi-LSTM\")(embeddings)\n",
        "dropout = Dropout(0.3, name='dropout')(bilstm)\n",
        "bilstm_out = Dense(len(tag2int), name='output')(dropout)\n",
        "output = Activation('softmax', name='softmax')(bilstm_out)\n",
        "\n",
        "model_bilstm = Model(inputs=[inputs], outputs=[output])\n",
        "model_bilstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJaNG7kqCna8"
      },
      "source": [
        "We just added a casual 400k parameters to our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZtH1BO9Cna8"
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 5\n",
        "\n",
        "model_bilstm.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy']\n",
        "             )\n",
        "\n",
        "history_bilstm = model_bilstm.fit(train_sentences, train_tags_1hot,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "df = pd.DataFrame(history_bilstm.history)\n",
        "df[['val_accuracy', 'accuracy']].plot.line();\n",
        "df[['val_loss', 'loss']].plot.line();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsmxXjjPCna-"
      },
      "source": [
        "loss, accuracy = model_bilstm.evaluate(test_sentences, test_tags_1hot,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzDYeZBfCnbA"
      },
      "source": [
        "We are getting better on the test data, what about the confusing sentences?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpUZ48lFCnbA"
      },
      "source": [
        "predictions_bilstm = model_bilstm.predict(strange_sentences_int)\n",
        "print(list(zip(strange_sentences, inverse_transform(strange_sentences, predictions_bilstm))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtR0ll_1CnbC"
      },
      "source": [
        "Not so much. We probably need a lot more data to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoS4Ye6VCnbC"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Get the large train, dev, and test data sets for CoNLL, apply the preprocessing steps and run the model on them. What performance do you get?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPQJ_qkfCnbD"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fgWVtxKCnbE"
      },
      "source": [
        "## Sequential API Model\n",
        "\n",
        "When you google for more details, you might come across the **Sequential API** (because we sequentially add layers, not because this is a sequence model). It does the same thing as the functional API, but in a different way. It is less flexible than the functional API we have been using, though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pli0isI7CnbE"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import InputLayer\n",
        "np.random.seed(42)\n",
        "\n",
        "model_seq = Sequential()\n",
        "model_seq.add(InputLayer(input_shape=(MAX_LENGTH, ), name=\"word_IDs\"))\n",
        "model_seq.add(Embedding(len(word2int), 128, mask_zero=True, name='embeddings'))\n",
        "model_seq.add(Bidirectional(LSTM(256, return_sequences=True), name='bi-LSTM'))\n",
        "model_seq.add(Dropout(0.3, name='dropout'))\n",
        "model_seq.add(Dense(len(tag2int), name='output'))\n",
        "model_seq.add(Activation('softmax', name='softmax'))\n",
        "model_seq.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaMCUFcyCnbG"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "model_seq.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model_seq.fit(train_sentences, train_tags_1hot,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Aeb2lOECnbI"
      },
      "source": [
        "loss, accuracy = model_seq.evaluate(test_sentences, test_tags_1hot,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvi-66CZCnbK"
      },
      "source": [
        "The sequential model does have a convenience function, though, `predict_classes()`, which we can use to get the tatg IDs. This changes our inverse transformation a bit, since we no longer need the argmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWcbfUznCnbL"
      },
      "source": [
        "predictions_seq = model_seq.predict_classes(strange_sentences_int)\n",
        "for sentence, prediction in zip(strange_sentences, predictions_seq):\n",
        "    pred_ = [int2tag[prediction[i]] for i in range(len(sentence))]\n",
        "    print(sentence, pred_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB1J3ZoVabv_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}