{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "len_10_Convolutional_Neural_Networks_and_BERT.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nF97c3fFJK1"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CeNo3U3FJK3"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "__2F8rUavbga",
        "outputId": "dc0bef8e-fa45-47de-b819-487ed9e05b29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWgNOGqyFJK4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "target = 'country'\n",
        "input_column = 'description_cleaned'\n",
        "\n",
        "data = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/wine_reviews-top4.xlsx')\n",
        "\n",
        "\n",
        "train_data = data[:45000]\n",
        "dev_data = data[45000:60000]\n",
        "test_data = data[60000:]\n",
        "\n",
        "train_instances = train_data[input_column].apply(str).apply(str.split)\n",
        "train_labels = train_data[target]\n",
        "\n",
        "# collect known word tokens and tags\n",
        "wordset, labelset = set(), set()\n",
        "\n",
        "# collect tags from all data, to prevent unseen labels\n",
        "labelset.update(set(data[target]))\n",
        "\n",
        "for words in train_instances:\n",
        "    wordset.update(set(words))\n",
        "\n",
        "# map words and tags into ints\n",
        "PAD = '-PAD-'\n",
        "UNK = '-UNK-'\n",
        "word2int = {word: i + 2 for i, word in enumerate(sorted(wordset))}\n",
        "word2int[PAD] = 0  # special token for padding\n",
        "word2int[UNK] = 1  # special token for unknown words\n",
        " \n",
        "label2int = {label: i for i, label in enumerate(sorted(labelset))}\n",
        "# to translate it back\n",
        "int2label = {i:label for label, i in label2int.items()}\n",
        "\n",
        "\n",
        "def convert2ints(instances):\n",
        "    result = []\n",
        "    for words in instances:\n",
        "        # replace words with int, 1 for unknown words\n",
        "        word_ints = [word2int.get(word, 1) for word in words]\n",
        "        result.append(word_ints)\n",
        "    return result\n",
        "                          \n",
        "train_instances_int = convert2ints(train_instances)\n",
        "train_labels_int = [label2int[label] for label in train_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulIdCD66dJd2"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUUTl23jFJK6"
      },
      "source": [
        "len(word2int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9uKZxDjFJK8"
      },
      "source": [
        "## Exercise\n",
        "Add a minimum frequency threshold for words, and replace any that are below with the `-UNK-` token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZvTf289FJK9"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDFGRSUFFuR3"
      },
      "source": [
        "## Convert Text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1QsufwNFJK_"
      },
      "source": [
        "# convert test data \n",
        "test_instances = test_data[input_column].apply(str).apply(str.split)\n",
        "test_labels = test_data[target]\n",
        "\n",
        "test_instances_int = convert2ints(test_instances)\n",
        "test_labels_int = [label2int[label] for label in test_labels]\n",
        "\n",
        "# convert dev data\n",
        "dev_instances = dev_data[input_column].apply(str).apply(str.split)\n",
        "dev_labels = dev_data[target]\n",
        "\n",
        "dev_instances_int = convert2ints(dev_instances)\n",
        "dev_labels_int = [label2int[label] for label in dev_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2po2l_gFJLA"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels_1hot = to_categorical(train_labels_int, len(label2int))\n",
        "test_labels_1hot = to_categorical(test_labels_int, len(label2int))\n",
        "dev_labels_1hot = to_categorical(dev_labels_int, len(label2int))\n",
        "\n",
        "train_labels_1hot[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMS7bHG2FJLC"
      },
      "source": [
        "# compute 95th percentile of training sentence lengths\n",
        "L = sorted(map(len, train_instances))\n",
        "MAX_LENGTH = L[int(len(L)*0.95)]\n",
        "print(MAX_LENGTH)\n",
        "\n",
        "# apply padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "train_instances_int = pad_sequences(train_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
        "test_instances_int = pad_sequences(test_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
        "dev_instances_int = pad_sequences(dev_instances_int, padding='post', maxlen=MAX_LENGTH)\n",
        "\n",
        "print(train_instances[0], len(train_instances[0]))\n",
        "print(train_instances_int[0], len(train_instances_int[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpKDNGiWFJLE"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEMMDYNCFJLF"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers import GlobalMaxPooling1D, Dropout\n",
        "from keras.layers.core import Dense, Activation\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# set parameters of matrices and convolution\n",
        "embedding_dim = 64\n",
        "nb_filter = 64\n",
        "filter_length = 3\n",
        "hidden_dims = 32\n",
        "stride_length = 1\n",
        "\n",
        "inputs = Input((MAX_LENGTH, ), \n",
        "               name='word_IDs')\n",
        "embeddings = Embedding(len(word2int), \n",
        "                       embedding_dim, \n",
        "                       input_length=MAX_LENGTH)(inputs)\n",
        "convolution = Conv1D(filters=nb_filter,  # Number of filters to use\n",
        "                    kernel_size=filter_length, # n-gram range of each filter.\n",
        "                    padding='same',  #valid: don't go off edge; same: use padding before applying filter\n",
        "                    activation='relu',\n",
        "                    strides=stride_length)(embeddings)\n",
        "convolution2 = Activation(activation='tanh')(convolution)\n",
        "pooling = GlobalMaxPooling1D()(convolution2)\n",
        "dropout1 = Dropout(0.2)(pooling)\n",
        "dense = Dense(hidden_dims, activation='relu')(dropout1)\n",
        "dropout2 = Dropout(0.2)(dense)\n",
        "output = Dense(len(label2int), activation='softmax')(dropout2)\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=[output])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7M2dYjjFJLH"
      },
      "source": [
        "# batch size can have a huge effect on performance!\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "history = model.fit(train_instances_int, train_labels_1hot,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(dev_instances_int, dev_labels_1hot)\n",
        "                   )\n",
        "\n",
        "loss, accuracy = model.evaluate(test_instances_int, test_labels_1hot,\n",
        "                                batch_size=batch_size,\n",
        "                                verbose=False)\n",
        "\n",
        "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxfDwiFiFJLJ"
      },
      "source": [
        "%matplotlib inline\n",
        "df = pd.DataFrame(history.history)\n",
        "df[['val_accuracy', 'accuracy']].plot.line();\n",
        "df[['val_loss', 'loss']].plot.line();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW44o4apFJLL"
      },
      "source": [
        "## Exercise\n",
        "Implement a simple n-gram Logistic Regression model and evaluate its performance with `classification report`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp-_5sFxFJLL"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIw8ckJEFJLR"
      },
      "source": [
        "## Exercise\n",
        "Transform the CNN predictions into integer labels and run the classification report on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkSyQHUzFJLR"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiSFMeZOeX0v"
      },
      "source": [
        "# BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izVGhXbkec7c"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfNzrptceY6H"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h89Mq26tebsz"
      },
      "source": [
        "outputs.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_DoWVIcfOdk"
      },
      "source": [
        "Extract BERT representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXziDlf1euL7"
      },
      "source": [
        "features = outputs['last_hidden_state'][:,0,:]\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny83lEG2fQlt"
      },
      "source": [
        "You can use ANY bert model for extracting the representation of a text! It is possible both to use multilingual models or models trained in other languages. You can check here for the complete list of models https://huggingface.co/models?filter=masked-lm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgJIQoCLfrgE"
      },
      "source": [
        "## Use fine-tuned BERT for text classification\n",
        "\n",
        "You can also use pre-trained BERT models already fine-tuned for text classification! For example, this is the list of the models fine-tuned for Sentiment Analysis https://huggingface.co/models?filter=sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDkqbbfie_yv"
      },
      "source": [
        "import torch\n",
        "import numpy as np \n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"MilaNLProc/feel-it-italian-sentiment\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"MilaNLProc/feel-it-italian-sentiment\")\n",
        "\n",
        "sentence = 'Oggi sono proprio contento!'\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "# Call the model and get the logits\n",
        "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "outputs = model(**inputs, labels=labels)\n",
        "loss, logits = outputs[:2]\n",
        "logits = logits.squeeze(0)\n",
        "\n",
        "# Extract probabilities\n",
        "proba = torch.nn.functional.softmax(logits, dim=0)\n",
        "\n",
        "# Unpack the tensor to obtain negative and positive probabilities\n",
        "negative, positive = proba\n",
        "print(f\"Probabilities: Negative {np.round(negative.item(),4)} - Positive {np.round(positive.item(),4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}